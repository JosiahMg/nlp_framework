{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join的使用方法\n",
    "可将list中的每个值使用join方法拼接成一个str类型  \n",
    "下面使用jieba.lcut()将list中的成员分词，返回一个list列表，然后使用join方法将分词后的list使用空格拼接成str类型\n",
    "\n",
    "##### 任务说明：  \n",
    "texts = ['我来到北京大学', '他来到了网易行研大厦', '小明硕士毕业于中国科学院', '我爱北京天安门']   \n",
    "将上面的列表中的字符串成员进行分词，分词后使用空格隔开。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer && TfidfVectorizer\n",
    "CounterVectorizer实现词袋模型\n",
    "\n",
    "(0, 5)\t1  \n",
    "0：表示texts的第一个句子\n",
    "5：表示词典的第6个词\n",
    "1：表示出现1词  \n",
    "\n",
    "##### 任务说明：  \n",
    "texts = ['我 来到 北京大学', '他 来到 了 网易 行研 大厦', '小明 硕士 毕业 于 中国 科学院', '我 爱 北京天安门']  \n",
    "将上面的列表使用CountVectorizer & TfidfVectorizer进行向量化表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = ['我 来到 北京大学', '他 来到 了 网易 行研 大厦', '小明 硕士 毕业 于 中国 科学院', '我 爱 北京天安门']\n",
    "\n",
    "# Counter方法\n",
    "# max_features表示最大词典数目\n",
    "vectorizer = CountVectorizer(max_features=10, ngram_range=(1, 2))\n",
    "\n",
    "data = vectorizer.fit_transform(texts)\n",
    "print(data)\n",
    "print(data.toarray())\n",
    "print(vectorizer.vocabulary_)  # 词典以及索引\n",
    "print(vectorizer.get_feature_names())  # 词典\n",
    "\n",
    "# Tfidf方法\n",
    "tfidf = TfidfVectorizer(smooth_idf=True, ngram_range=(1, 2))\n",
    "data = tfidf.fit_transform(texts).toarray()\n",
    "print(data.shape)\n",
    "print(data)\n",
    "print(tfidf.get_feature_names())\n",
    "for k, v in tfidf.vocabulary_.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame()生成csv文件  \n",
    "\n",
    "##### 任务说明：  \n",
    "data = np.array([12, 3, 3, 12, 234, 83])  \n",
    "使用pandas将数据保存到csv文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-3\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-04(+1)~~  \n",
    "~~2020-12-05(+2)~~  \n",
    "~~2020-12-07(+4)  \n",
    "2020-12-10(+7)  \n",
    "2020-12-18(+15)~~  \n",
    "2021-01-03(+30)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明：  \n",
    "将下面的第一行数据变成第二行数据，再变回来。  \n",
    "[[1, 2, 3], [3, 5, 6]]  \n",
    "[\"1 2 3\", \"3 5 6\"]  \n",
    "[[1, 2, 3], [3, 5, 6]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, 2, 3], [3, 5, 6]])\n",
    "\n",
    "# 使用astype函数将list中的int变成str类型\n",
    "data = [\" \".join(d.astype(str)) for d in data]\n",
    "print(data)\n",
    "\n",
    "# 通过split分隔str，然后转换为ndarray\n",
    "data = np.array([d.split(\" \") for d in data]).astype(int)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-7\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-08(+1)  \n",
    "2020-12-09(+2)  \n",
    "2020-12-11(+4)  \n",
    "2020-12-14(+7)  \n",
    "2020-12-22(+15)~~  \n",
    "2021-01-07(+30)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明：  \n",
    "将十进制数转换成二进制的数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy \n",
    "\n",
    "# 设置最多转换多少个二进制数\n",
    "NUM_DIGITS = 10\n",
    "\n",
    "def binary_encode(i, num_digits=NUM_DIGITS):\n",
    "    return [i>>d & 1 for d in range(num_digits)][::-1]  ## [::-1]表示取反\n",
    "\n",
    "binary_encode(31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明：  \n",
    "将torch的Embedding()的初始化权重设置在-0.2 - +0.2之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "emb = torch.nn.Embedding(10, 2)\n",
    "print(emb.weight.data)\n",
    "\n",
    "emb.weight.data.uniform_(-0.2, 0.2)\n",
    "print(emb.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-10\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-11(+1)~~  \n",
    "~~2020-12-12(+2)  \n",
    "2020-12-14(+4)  \n",
    "2020-12-17(+7)  \n",
    "2020-12-25(+15)~~  \n",
    "2021-01-10(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "使用gensim训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dogs'], ['yoyoyo', 'you', 'go', 'home', 'now', 'to', 'sleep']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import pandas as pd\n",
    "\n",
    "num_features = 100     # Word vector dimensionality\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "\n",
    "raw_sentences = [\"the quick brown fox jumps over the lazy dogs\", \"yoyoyo you go home now to sleep\"]\n",
    "\n",
    "sentences = [s.split() for s in raw_sentences]\n",
    "print(sentences)\n",
    "\n",
    "df = pd.read_csv('../corpus/tianchi_news/train_set.csv', sep='\\t')\n",
    "df['train_text'] = df.text.apply(lambda text: text.split())\n",
    "\n",
    "\n",
    "# sentences的结构必须是iterable of iterables, 即下面类型的：\n",
    "# [[], [], [], []]\n",
    "# 通过两个for循环取数据\n",
    "# min_count必须指定\n",
    "# model = word2vec.Word2Vec(sentences, window=5, min_count=1, size=num_features, workers=num_workers)\n",
    "model = word2vec.Word2Vec(df['train_text'], window=5, min_count=1, size=num_features, workers=num_workers)\n",
    "\n",
    "# 计算l2范数 模型可以节省空间 但是生成的模型不能再次被训练\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "\n",
    "\n",
    "# save model\n",
    "model.save(\"./word2vec.bin\")\n",
    "\n",
    "\n",
    "# load model\n",
    "model = word2vec.Word2Vec.load(\"./word2vec.bin\")\n",
    "\n",
    "model.save(\"./word2vec.bin\")\n",
    "# convert format\n",
    "model.wv.save_word2vec_format('./word2vec.txt', binary=False)\n",
    "\n",
    "print(model.wv.similarity('dogs','you'))\n",
    "\n",
    "print(model.wv['dogs'][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "shuffle操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "data = [['hello world-1'], ['hello world-2'], ['hello world-3'], ['hello world-4'], ['hello world-5'], \n",
    "        ['hello world-6'], ['hello world-7'], ['hello world-8'], ['hello world-9'], ['hello world-10']]\n",
    "\n",
    "\n",
    "# 使用np.random.shuffle 直接在原数据上打乱\n",
    "index = list(range(10))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# 使用sklearn中的shuffle 不会在原数据上修改\n",
    "index = list(range(10))\n",
    "index = shuffle(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "N-折数据\n",
    "读取./corpus/tianchi_news/train_set.csv文件，然后进行10折数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import pandas as pd\n",
    "\n",
    "df_train_all = pd.read_csv('../corpus/tianchi_news/train_set.csv', sep='\\t', nrows=1000)\n",
    "print(len(df_train_all))\n",
    "print(df_train_all.columns)\n",
    "\n",
    "gkf = GroupKFold(n_splits=10).split(X=df_train_all, groups=df_train_all.index)\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        df_train = df_train_all.iloc[train_idx]\n",
    "        df_valid = df_train_all.iloc[valid_idx]\n",
    "        print(f'Train data shape: {df_train.shape}')\n",
    "        print(f'Train positive data shape: {df_train[df_train[\"label\"] == 2].shape}')\n",
    "        print(f'Validation data shape: {df_valid.shape}')\n",
    "        print(f'Validation positive data shape: {df_valid[df_valid[\"label\"] == 2].shape}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "list中str成员的split()操作的种方法：\n",
    "map() & [ for ..]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['This is a demo', 'I can do it', 'come on, you can!']\n",
    "\n",
    "data1 = [d.split() for d in data]\n",
    "data2 = list(map(lambda x: list(x.split()), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ['This is a demo', 'I can do it', 'come on, you can!']\n",
    "\n",
    "func = lambda x: list(x.split())\n",
    "func(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-11\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-12(+1)  \n",
    "2020-12-14(+2)  \n",
    "2020-12-15(+4)  \n",
    "2020-12-18(+7)  \n",
    "2020-12-26(+15)~~  \n",
    "2021-01-11(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "### 迭代器\n",
    "\n",
    "迭代器有两个基本的方法：iter() 和 next()。\n",
    "\n",
    "字符串，列表或元组对象都可用于创建迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单的迭代器\n",
    "\n",
    "it = (x*2 for x in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interator\n",
    "\n",
    "list = [1, 2, 3, 4]\n",
    "it = iter(list)\n",
    "\n",
    "# 使用for语句遍历迭代器时,迭代器会自动初始化\n",
    "for x in it:\n",
    "    print(x)\n",
    "    \n",
    "import sys\n",
    "# 使用next()遍历时,必须设置异常\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it))\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "创建一个迭代器 需要实现两个函数 __iter__() 和 __next__()方法\n",
    "__iter__() 方法返回一个特殊的迭代器对象， 这个迭代器对象实现了 __next__() 方法并通过 StopIteration 异常标识迭代的完成。\n",
    "__next__() 方法（Python 2 里是 next()）会返回下一个迭代器对象。\n",
    "StopIteration 异常用于标识迭代的完成，防止出现无限循环的情况\n",
    "创建一个返回数字的迭代器，初始值为 1，逐步递增 1\n",
    "\"\"\"\n",
    "class MyNumbers:\n",
    "    # 使用iter()函数以及for循环的第一次时会调用\n",
    "    def __iter__(self):\n",
    "        self.a = 1\n",
    "        return self\n",
    "    \n",
    "    # 使用next()函数以及for循环时会调用\n",
    "    def __next__(self):\n",
    "        if self.a <= 10:\n",
    "            x = self.a\n",
    "            self.a += 1\n",
    "            return x\n",
    "        else:\n",
    "            raise StopIteration()\n",
    "    \n",
    "num = MyNumbers()\n",
    "it = iter(num)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(next(it))\n",
    "    except StopIteration:\n",
    "        break\n",
    "        \n",
    "for data in it:\n",
    "    print(data)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成器  yield\n",
    "\n",
    "##### yield的主要作用是可以不用将数据读入到内存中,使用迭代器保存\n",
    "\n",
    "在 Python 中，使用了 yield 的函数被称为生成器（generator）。\n",
    "\n",
    "跟普通函数不同的是，生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。\n",
    "\n",
    "在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行。\n",
    "\n",
    "调用一个生成器函数，返回的是一个迭代器对象。\n",
    "\n",
    "yield 的作用就是把一个函数变成一个 generator，带有 yield 的函数不再是一个普通函数，Python 解释器会将其视为一个 generator，调用 fab(5) 不会执行 fab 函数，而是返回一个 iterable 对象！在 for 循环执行时，每次循环都会执行 fab 函数内部的代码，执行到 yield b 时，fab 函数就返回一个迭代值，下次迭代时，代码从 yield b 的下一条语句继续执行，而函数的本地变量看起来和上次中断执行前是完全一样的，于是函数继续执行，直到再次遇到 yield。  \n",
    "在一个 generator function 中，如果没有 return，则默认执行至函数完毕，如果在执行过程中 return，则直接抛出 StopIteration 终止迭代。  \n",
    "\n",
    "\n",
    "以下实例使用 yield 实现斐波那契数列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Iterator\n",
    " \n",
    "def fibonacci(n): # 生成器函数 - 斐波那契\n",
    "    a, b, counter = 0, 1, 0\n",
    "    while True:\n",
    "        if (counter > n): \n",
    "            return\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "f = fibonacci(10) # f 是一个迭代器，由生成器返回生成\n",
    "\n",
    "# 判断一个类是否是迭代器的方法\n",
    "print(isinstance(f, Iterator))\n",
    "print(isinstance(fibonacci, Iterator))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        print(next(f), end=\" \")\n",
    "    except StopIteration:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-13\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-14(+1)  \n",
    "2020-12-15(+2)  \n",
    "2020-12-17(+4)  \n",
    "2020-12-20(+7)  \n",
    "2020-12-28(+15)~~  \n",
    "2021-01-13(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counter的两种使用方法\n",
    "\n",
    "统计下面的字符串中的字符\n",
    "\n",
    "text = 'Our legislation on participating in a bid is clear: no one can be prohibited, said Mourao, adding the only thing the company must do is to demonstrate its transparency (in keeping) with the rules that will be established for the process.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 使用nltk.word_tokenzize()比 split()的好处在于, split()无法将\"clear:\"分开\n",
    "import nltk\n",
    "\n",
    "MAX_VOCAB_SIZE = 20\n",
    "\n",
    "text = 'Our legislation on participating in a bid is clear: no one can be prohibited, said Mourao, adding the only thing the company must do is to demonstrate its transparency (in keeping) with the rules that will be established for the process.'\n",
    "\n",
    "\n",
    "text = nltk.word_tokenize(text)\n",
    "\n",
    "# 方法一\n",
    "ls = Counter(text)\n",
    "# ls.most_common()返回list类型\n",
    "ls = ls.most_common(MAX_VOCAB_SIZE)\n",
    "\n",
    "vocab = {w[0]: index for index, w in enumerate(ls, 2)}\n",
    "\n",
    "\n",
    "\n",
    "# 方法二\n",
    "counter = Counter()\n",
    "for word in text:\n",
    "    counter[word] += 1\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "counter = np.array(counter.most_common(MAX_VOCAB_SIZE))\n",
    "\n",
    "reverse = lambda x: dict(zip(x[:, 0], range(len(x))))\n",
    "\n",
    "vocab = reverse(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-14\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-15(+1)  \n",
    "2020-12-16(+2)  \n",
    "2020-12-18(+4)  \n",
    "2020-12-21(+7)  \n",
    "2020-12-29(+15)~~  \n",
    "2021-01-14(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据的K-Fold函数\n",
    "\n",
    "GroupKFold：  \n",
    "传入的参数是一个整体,这个整体包含了X和y数据\n",
    "\n",
    "StratifiedKFold:\n",
    "传入参数是X和y两组数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('../corpus/iris.data.txt', sep=',', header=None)\n",
    "df = df.iloc[-10:]\n",
    "\n",
    "gkf = GroupKFold(n_splits=10).split(X=df, groups=df.index)\n",
    "\n",
    "for k, (train_idx, test_idx) in enumerate(gkf):\n",
    "    train_X = df.iloc[train_idx, [2, 3]]\n",
    "    train_y = df.iloc[train_idx, 4]\n",
    "    test_X = df.iloc[test_idx, [2, 3]]\n",
    "    test_y = df.iloc[test_idx, 4]\n",
    "    print(train_X.shape, train_y.shape)\n",
    "\n",
    "    print(test_X.shape, test_y.shape)\n",
    "    break\n",
    "\n",
    "\n",
    "    \n",
    "X = df[[2, 3]]\n",
    "y = df[4]\n",
    "kfold = StratifiedKFold(n_splits=10).split(X, y)\n",
    "\n",
    "for k, (train_idx, test_idx) in enumerate(kfold):\n",
    "    train_X = X.iloc[train_idx]\n",
    "    train_y = y.iloc[train_idx]\n",
    "    \n",
    "    test_X = X.iloc[test_idx]\n",
    "    test_y = y.iloc[test_idx]\n",
    "    print(train_X.shape, train_y.shape)\n",
    "    print(test_X.shape, test_y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 混淆矩阵\n",
    "\n",
    "|actual\\predict| P  | N |\n",
    "|  :----  | ----  |----  |\n",
    "| P  | TP |FN|\n",
    "| N  | FP |TN |  \n",
    "\n",
    "#### 准确率 accuracy\n",
    "\n",
    "accuracy = $\\frac{TP+FP}{TP+FP+FN+TN}$\n",
    "\n",
    "#### 精确度 precision\n",
    "\n",
    "precision = $\\frac{TP}{TP+FP}$\n",
    "\n",
    "#### 召回率/敏感度/真正率  recall/sensitivity\n",
    "recall = $\\frac{TP}{TP+FN}$\n",
    "\n",
    "#### 特异度\\真负率  specificity\n",
    "specificity = $\\frac{TN}{TN+FP}$\n",
    "\n",
    "#### F1-score\n",
    "F1-score = $\\frac{2*precison*recall}{precision+recall}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../corpus/iris.data.txt', header=None)\n",
    "X = df.iloc[-100:, [2, 3]].values  #numpy类型\n",
    "y = df.iloc[-100:, 4].values    #numpy类型\n",
    "\n",
    "# 对标签进行编码\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10).split(X, y)\n",
    "\n",
    "for k, (train_idx, val_idx) in enumerate(kfold):\n",
    "    model = LogisticRegression(C=100., random_state=1)\n",
    "    model.fit(X[train_idx], y[train_idx])\n",
    "    y_pred = model.predict(X[val_idx])\n",
    "    y_proba = model.predict_proba(X[val_idx])[:, 1]\n",
    "    # 混淆矩阵\n",
    "    confmat = confusion_matrix(y_true=y[val_idx], y_pred=y_pred)\n",
    "    # 准确率\n",
    "    accuracy = model.score(X[val_idx], y[val_idx])\n",
    "    # 精确度\n",
    "    precision = precision_score(y_true=y[val_idx], y_pred=y_pred)\n",
    "    # 召回率\n",
    "    recall = recall_score(y_true=y[val_idx], y_pred=y_pred)\n",
    "    # f1-socre\n",
    "    f1 = f1_score(y_true=y[val_idx], y_pred=y_pred)\n",
    "    # 特异度\n",
    "    tn = confmat[0, 0]\n",
    "    fp = confmat[0, 1]\n",
    "    specificity = float(tn)/(float(tn)+float(fp))\n",
    "    # AUC\n",
    "    print(y[val_idx])\n",
    "    print(y_proba)\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y[val_idx], y_score=y_proba)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "\n",
    "    print(f'accuracy={accuracy}, precision={precision}, recall={recall}, f1_score={f1}, specificity={specificity}, auc={auc_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-15\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-16(+1)  \n",
    "2020-12-17(+2)  \n",
    "2020-12-18(+4)  \n",
    "2020-12-22(+7)  \n",
    "2020-12-30(+15)~~  \n",
    "2021-01-15(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 任务说明： \n",
    "### 构造vocabulary\n",
    "\n",
    "使用tianchi_news下面的数据构造一个词典：  \n",
    "主要包含两个词典,一个是dict类型的word2id, 一个list类型的id2word, 每种类型包括text和label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "\n",
    "df_all = pd.read_csv('../corpus/tianchi_news/train_set.csv', sep='\\t')\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, df, min_count=5):\n",
    "        self.min_count = min_count\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "        self.build_vocab(df)\n",
    "        \n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "        \n",
    "    def build_vocab(self, df):\n",
    "        self.word_counter = Counter()\n",
    "        for text in df['text']:\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counter[word] += 1\n",
    "        \n",
    "        for word, count in self.word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "        \n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "        \n",
    "        self.label_counter = Counter(df['label'])\n",
    "        \n",
    "        for label in range(len(self.label_counter)):\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "    \n",
    "    def word2id(self, xs):\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "    \n",
    "    def label2id(self, xs):\n",
    "        if isinstance(data, (list, np.ndarray)):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "    \n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._label2id)\n",
    "    \n",
    "# vocab = Vocab(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-20\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-21(+1)  \n",
    "2020-12-22(+2)  \n",
    "2020-12-24(+4)  \n",
    "2020-12-25  \n",
    "2020-12-27(+7)  \n",
    "2020-01-04(+15)~~  \n",
    "2021-01-20(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCnn模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, emb_size, out_channels, vocab_size, num_classes, filter_sizes=[2, 3, 4], drop=0.2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, out_channels, (k, emb_size)) for k in filter_sizes])\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.fc = nn.Linear(len(filter_sizes)*out_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x):  # x.shape: (batch_size, seq_len)\n",
    "        x = self.emb(x)  # x.shape: (batch_size, seq_len, emb_size)\n",
    "        x = x.unqueeze(1) # x.shape: (batch_size, 1, seq_len, emb_size)\n",
    "        \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # len(self.convs)*(batch_size, out_channels, w)\n",
    "        x = [F.max_pool1d(line, line.size(2)).squeeze(2) for line in x] # len(self.convs)*(batch_size, out_channels)\n",
    "        x = torch.cat(x, dim=1)  # (batch, len(self.convs)*out_channels)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maxpool的使用\n",
    "\n",
    "- kernel_size – the size of the window to take a max over\n",
    "- stride – the stride of the window. Default value is kernel_size\n",
    "- padding – implicit zero padding to be added on both sides\n",
    "\n",
    "### F.max_pool1d(input_data, kernel_size, ...)  & nn.MaxPool1d(kernel_size, ...)\n",
    "Input: (N, C, L_in)  \n",
    "Output: (N,C, L_out)\n",
    "\n",
    "### F.max_pool2d(input_data, kernel_size, ...) & nn.MaxPool2d(kernel_size, ...)\n",
    "Input: (N, C, H_in, W_in)  \n",
    "Output: (N,C,H_out, W_out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "input = torch.randn(20, 16, 50)\n",
    "\n",
    "F.max_pool1d(input, input.size(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "m = nn.MaxPool2d(3, stride=2)\n",
    "# pool of non-square window\n",
    "m = nn.MaxPool2d((3, 2), stride=(2, 1))\n",
    "input = torch.randn(20, 16, 50, 32)\n",
    "m(input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "data = [[1, 3], [12, 34, 3, 4], [234]]\n",
    "\n",
    "seq_len = 3\n",
    "\n",
    "lens = [len(text) if len(text)<seq_len else seq_len for text in data]\n",
    "\n",
    "inputs = torch.zeros((len(data)), seq_len, dtype=torch.float32)\n",
    "for i, text in enumerate(data):\n",
    "    inputs[i, :lens[i]] = torch.LongTensor(data[i][:seq_len])\n",
    "    \n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-22\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-23(+1)  \n",
    "2020-12-24(+2)  \n",
    "2020-12-26(+4)  \n",
    "2020-12-29(+7)~~  \n",
    "2020-01-06(+15)  \n",
    "2021-01-22(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, n_features=2):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = torch.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model_name = 'checkpoint.bin'\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "check_point = {'model': model.state_dict(), 'lr': 0.01, 'epoch': 2}\n",
    "\n",
    "\n",
    "# 保存GPU上的模型\n",
    "torch.save(check_point, model_name)\n",
    "\n",
    "# 加载模型到cpu上\n",
    "model_cpu = Model()\n",
    "\n",
    "# 使用state_dict()保存的模型, 参数map_location会失效\n",
    "check = torch.load(model_name, map_location=torch.device('cpu'))\n",
    "\n",
    "model_cpu.load_state_dict(check['model'])\n",
    "print(next(model_cpu.parameters()).is_cuda)\n",
    "\n",
    "# 读取epcoh和lr\n",
    "print(check['lr'])\n",
    "print(check['epoch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-23\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-24(+1)  \n",
    "2020-12-25(+2)  \n",
    "2020-12-27(+4)  \n",
    "2020-12-30(+7)~~  \n",
    "2020-01-07(+15)  \n",
    "2021-01-23(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率衰减\n",
    "两种方法：\n",
    "- 直接修改optimizer的参数  \n",
    "该方法可以定义一个函数,每次修改lr时调用该函数即可\n",
    "\n",
    "    def init_optimizer(self, lr):\n",
    "        self.optimizer = torch.optim.Adam(self.optim_parameters, lr=lr, weight_decay=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LR = 0.01\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])), lr=LR)\n",
    "print(optimizer.param_groups[0].keys())\n",
    "\n",
    "lr_list = []\n",
    "for epoch in range(100):\n",
    "    if epoch % 5 == 0:\n",
    "        for p in optimizer.param_groups:\n",
    "            p['lr'] *= 0.9\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  利用pytorch提供的lr_scheduler()函数  \n",
    "\n",
    "### torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)  \n",
    "lr_lambda 会接收到一个int参数：epoch，然后根据epoch计算出对应的lr。如果设置多个lambda函数的话，会分别作用于Optimizer中的不同的params_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "lr_list = []\n",
    "\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])),lr = 0.1)\n",
    "# lambda1 = lambda epoch: np.sin(epoch)/epoch\n",
    "\n",
    "decay = .75\n",
    "lambda1 = lambda step: decay ** (step // 10)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
    "iiii\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)  \n",
    "每经过step_size步骤, lr会自动乘以gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "lr_list = []\n",
    "\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])),lr = 0.1)\n",
    "scheduler = lr_scheduler.StepLR(optimizer,step_size=5,gamma = 0.8)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)  \n",
    "三段式lr，epoch进入milestones范围内即乘以gamma，离开milestones范围之后再乘以gamma\n",
    "这种衰减方式也是在学术论文中最常见的方式，一般手动调整也会采用这种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "lr_list = []\n",
    "\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])),lr = 0.1)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[20,80], gamma = 0.8)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)  \n",
    "指数衰减"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "lr_list = []\n",
    "\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])),lr = 0.1)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma = 0.8)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)  \n",
    "- T_max 对应1/2个cos周期所对应的epoch数值\n",
    "- eta_min 为最小的lr值，默认为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "lr_list = []\n",
    "optimizer = torch.optim.Adam(iter(torch.tensor([1., 2.])),lr = 0.1)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max = 20)\n",
    "\n",
    "for epoch in range(100):\n",
    "    scheduler.step()\n",
    "    lr_list.append(optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "plt.plot(range(100), lr_list, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-25\n",
    "\n",
    "### Counterdown  \n",
    "~~2020-12-26(+1)  \n",
    "2020-12-27(+2)  \n",
    "2020-12-29(+4)~~  \n",
    "2020-01-01(+7)  \n",
    "2020-01-09(+15)  \n",
    "2021-01-25(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_file = '../corpus/tianchi_news/train_set.csv'\n",
    "    test_file = '../corpus/tianchi_news/test_a.csv'\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "\n",
    "    # 读取文件中的数据\n",
    "    df_all = pd.read_csv(train_file, sep='\\t', nrows=100)\n",
    "    df_test = pd.read_csv(test_file, sep='\\t', nrows=100)\n",
    "    df_test['label'] = np.zeros(len(df_test), dtype=np.int)\n",
    "\n",
    "    # 构建词典\n",
    "    vocab = NewsVocab(df_all)\n",
    "\n",
    "    # 训练集和验证集数据\n",
    "    df_train, df_val = train_test_split(df_all, test_size=0.2, random_state=10)\n",
    "\n",
    "\n",
    "\n",
    "    #预处理数据\n",
    "    train_data = preprocess(df_train, vocab)\n",
    "    val_data = preprocess(df_val, vocab)\n",
    "    test_data = preprocess(df_test, vocab)\n",
    "\n",
    "\n",
    "    train_dataset = NewsDataset(train_data)\n",
    "    val_dataset = NewsDataset(val_data)\n",
    "    test_dataset = NewsDataset(test_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    emb_size = 100\n",
    "    out_channels = 100\n",
    "    vocab_size = vocab.word_size\n",
    "    num_classes = vocab.label_size\n",
    "    lr = 1e-2\n",
    "    print(f'vocabulary size is {vocab_size}, num_classes is {num_classes}')\n",
    "    # 创建模型 优化器  损失函数\n",
    "    model = TextCNN(emb_size, out_channels, vocab_size, num_classes)\n",
    "    optimizer = Optimizer(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, loss_fn, state_dict_path='./state_dict')\n",
    "\n",
    "    trainer.fit(train_dataloader, val_dataloader)\n",
    "    trainer.predict(test_dataloader, save_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词典\n",
    "- 输入格式： 分词后的list或者ndarray类型的数据经过DataProcess.preprocess()处理后的数据  \n",
    "{'text': texts, 'label': label} or pd.read_csv()  \n",
    "texts: [[word, word, ...], [word, word, ...], ...]  \n",
    "labels: [index, index, ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedKFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import tqdm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVocab():\n",
    "    \"\"\"\n",
    "    构建字典:\n",
    "    train_data.keys(): ['text', 'label']\n",
    "    train_data['text']: iterable of iterables\n",
    "    train_data['label']: iterable\n",
    "    \"\"\"\n",
    "    def __init__(self, train_data, min_count=5):\n",
    "        self.min_count = min_count\n",
    "        self.pad = 0\n",
    "        self.unk = 1\n",
    "        self._id2word = ['[PAD]', '[UNK]']\n",
    "\n",
    "        self._id2label = []\n",
    "        self.target_names = []\n",
    "\n",
    "        self.build_vocab(train_data)\n",
    "\n",
    "        reverse = lambda x: dict(zip(x, range(len(x))))\n",
    "        self._word2id = reverse(self._id2word)\n",
    "        self._label2id = reverse(self._id2label)\n",
    "\n",
    "        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n",
    "\n",
    "    def build_vocab(self, data):\n",
    "        word_counter = Counter()\n",
    "\n",
    "        for words in data['text']:\n",
    "            words = words.strip().split()\n",
    "            for word in words:\n",
    "                word_counter[word] += 1\n",
    "\n",
    "        for word, count in word_counter.most_common():\n",
    "            if count >= self.min_count:\n",
    "                self._id2word.append(word)\n",
    "\n",
    "        label2name = {0: '科技', 1: '股票', 2: '体育', 3: '娱乐', 4: '时政', 5: '社会', 6: '教育', 7: '财经',\n",
    "                      8: '家居', 9: '游戏', 10: '房产', 11: '时尚', 12: '彩票', 13: '星座'}\n",
    "\n",
    "        label_counter = Counter(data['label'])\n",
    "\n",
    "        for label in range(len(label_counter)):\n",
    "            count = label_counter[label]\n",
    "            self._id2label.append(label)\n",
    "            self.target_names.append(label2name[label])\n",
    "\n",
    "\n",
    "    def word2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._word2id.get(x, self.unk) for x in xs]\n",
    "        return self._word2id.get(xs, self.unk)\n",
    "\n",
    "\n",
    "    def label2id(self, xs):\n",
    "        if isinstance(xs, list):\n",
    "            return [self._label2id.get(x, self.unk) for x in xs]\n",
    "        return self._label2id.get(xs, self.unk)\n",
    "\n",
    "    @property\n",
    "    def word_size(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def label_size(self):\n",
    "        return len(self._id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-30\n",
    "\n",
    "### Counterdown   \n",
    "2020-12-31(+1)  \n",
    "2020-01-01(+2)  \n",
    "2020-01-03(+4)  \n",
    "2020-01-06(+7)  \n",
    "2020-01-14(+15)  \n",
    "2021-01-30(+30)\n",
    "\n",
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, vocab):\n",
    "    texts = df['text'].to_list()\n",
    "    labels = df['label'].to_list()\n",
    "\n",
    "    texts = [vocab.word2id(text.strip().split()) for text in texts]\n",
    "    labels = vocab.label2id(labels)\n",
    "    return {'text': texts, 'label': labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-30\n",
    "### Counterdown  \n",
    "2020-12-31(+1)  \n",
    "2020-01-01(+2)  \n",
    "2020-01-03(+4)  \n",
    "2020-01-06(+7)  \n",
    "2020-01-14(+15)  \n",
    "2021-01-30(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用torch.utils.data模块创建数据\n",
    "\n",
    "- 输入： 将之前模块的输出\n",
    "- 处理： 将所有的句子统一相同的长度\n",
    "- 返回： tensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, seq_len=256):\n",
    "        inputs = df['text']\n",
    "        self.labels = torch.LongTensor(df['label'])\n",
    "        self.lens = torch.LongTensor([len(text) if len(text)<seq_len else seq_len for text in inputs])\n",
    "        # 统一相同的长度并变成LongTensor类型\n",
    "        self.inputs = torch.zeros(len(inputs), seq_len, dtype=torch.int64)\n",
    "        for i, text in enumerate(self.inputs):\n",
    "            self.inputs[i, :self.lens[i]] = torch.LongTensor(inputs[i][:seq_len])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {'text': self.inputs[item], 'len': self.lens[item], 'label': self.labels[item]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-31\n",
    "### Counterdown  \n",
    "2020-01-01(+1)  \n",
    "2020-01-02(+2)  \n",
    "2020-01-04(+4)  \n",
    "2020-01-07(+7)  \n",
    "2020-01-15(+15)  \n",
    "2021-01-31(+30)\n",
    "\n",
    "###  构建模型\n",
    "- 模型类,构造模型结构\n",
    "- 优化器构造\n",
    "- trainer类型, 训练 预测 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, emb_size, out_channels, vocab_size, num_classes, filter_sizes=[2, 3, 4], drop=0.2):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, out_channels, (k, emb_size)) for k in filter_sizes])\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.fc = nn.Linear(len(filter_sizes) * out_channels, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x.shape: (batch_size, seq_len)\n",
    "        x = self.emb(x)  # x.shape: (batch_size, seq_len, emb_size)\n",
    "        x = x.unsqueeze(1)  # x.shape: (batch_size, 1, seq_len, emb_size)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]  # len(self.convs)*(batch_size, out_channels, w)\n",
    "        x = [F.max_pool1d(line, line.size(2)).squeeze(2) for line in x]  # len(self.convs)*(batch_size, out_channels)\n",
    "        x = torch.cat(x, dim=1)  # (batch, len(self.convs)*out_channels)\n",
    "        x = self.dropout(x)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model_parameters, lr, weight_decay=0, lr_scheduler=False):\n",
    "        self.optimizer = torch.optim.Adam(model_parameters, lr=lr, weight_decay=weight_decay)\n",
    "        # 每decay_step个epoch降低LR一次\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        if self.lr_scheduler:\n",
    "            lr_func = lambda step: 0.75 ** (step // 100)\n",
    "            self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lr_func)\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "        if self.lr_scheduler:\n",
    "            self.scheduler.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def get_lr(self):\n",
    "        return self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "\n",
    "    # 手动更新学习率, 每次更新后变成原来的decay倍\n",
    "    def update_lr(self, decay=0.9):\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] *= decay\n",
    "        print(f'Update LR to {self.get_lr()}')\n",
    "\n",
    "    # 用于模型的保存和加载\n",
    "    def state_dict(self):\n",
    "        return self.optimizer.state_dict()\n",
    "\n",
    "    def load_state_dict(self, chkpoint):\n",
    "        return self.optimizer.load_state_dict(chkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-01-01\n",
    "### Counterdown  \n",
    "2020-01-02(+1)  \n",
    "2020-01-03(+2)  \n",
    "2020-01-05(+4)  \n",
    "2020-01-08(+7)  \n",
    "2020-01-16(+15)  \n",
    "2021-02-01(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer类中构造优化器 等功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    state_dict_path: 模型保存路径\n",
    "    prefix_model_name: 模型名称前缀\n",
    "    \"\"\"\n",
    "    def __init__(self, model, optimizer, loss_fn, state_dict_path='./state_dict', prefix_model_name='textcnn.model'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.state_dict_path = state_dict_path\n",
    "        self.prefix_model_name = prefix_model_name\n",
    "        self.model = model.to(self.device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def fit(self, train_dataloader, val_dataloader, epoch_nums=999, patient=3, resume=False):\n",
    "        if resume and self.check_checkpoint():\n",
    "            start_epoch = self.load_model()\n",
    "            start_epoch += 1\n",
    "            print(f'Load checkpoint to continue training: start_epoch: {start_epoch}, lr: {self.optimizer.get_lr()}')\n",
    "\n",
    "        else:\n",
    "            start_epoch = 1\n",
    "\n",
    "        all_loss = []\n",
    "        all_acc = []\n",
    "        threshold = 0\n",
    "        for epoch in range(start_epoch, epoch_nums + 1):\n",
    "            self.train(epoch, train_dataloader)\n",
    "            loss, acc = self.test(epoch, val_dataloader)\n",
    "\n",
    "            all_loss.append(loss)\n",
    "            all_acc.append(acc)\n",
    "\n",
    "            best_acc = max(all_acc)\n",
    "            best_loss = min(all_loss)\n",
    "\n",
    "            if all_acc[-1] < best_acc:\n",
    "                threshold += 1\n",
    "                self.optimizer.update_lr(decay=0.8)\n",
    "\n",
    "            else:\n",
    "                self.save_state_dict(epoch, best_acc, best_acc)\n",
    "                threshold = 0\n",
    "\n",
    "            if threshold >= patient:\n",
    "                print(\"epoch {} has the lowest loss\".format(start_epoch + np.argmax(np.array(all_acc))))\n",
    "                print(\"early stop!\")\n",
    "                break\n",
    "\n",
    "    def train(self, epoch, train_dataloader):\n",
    "        self.model.train()\n",
    "        self._iteration(epoch, train_dataloader)\n",
    "\n",
    "    def test(self, epoch, val_dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            return self._iteration(epoch, val_dataloader, train=False)\n",
    "\n",
    "    def _iteration(self, epoch, dataloader, train=True):\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "        data_iter = tqdm.tqdm(enumerate(dataloader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(dataloader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        total_loss = 0.\n",
    "        corrects = 0\n",
    "        total_samples = 0\n",
    "        for i, data in data_iter:\n",
    "            inputs = data['text'].to(self.device)  # (batch, seq_len)\n",
    "            targets = data['label'].to(self.device)  # (batch)\n",
    "\n",
    "            outputs = self.model(inputs)  # (batch, num_classes)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "\n",
    "            # 训练模式进行反向传播\n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # 多分类任务\n",
    "            result = torch.max(outputs, 1)[1].view(targets.size())  # (batch)\n",
    "            # 计算准确率\n",
    "            corrects += (result == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if train:\n",
    "                log_dic = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": total_loss / (i + 1), \"train_acc\": corrects / total_samples,\n",
    "                    \"test_loss\": 0, \"test_acc\": 0\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                log_dic = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": 0, \"train_acc\": 0,\n",
    "                    \"test_loss\": total_loss / (i + 1), \"test_acc\": corrects / total_samples\n",
    "                }\n",
    "\n",
    "            # 打印日志信息\n",
    "            if (i + 1) % 100 == 0 or (i + 1) == len(data_iter):\n",
    "                data_iter.write(str({k: v for k, v in log_dic.items() if v != 0}))\n",
    "\n",
    "\n",
    "        return total_loss / len(data_iter), corrects / total_samples\n",
    "\n",
    "    def predict(self, test_dataloader, save_file=False):\n",
    "        model_name = self.find_most_recent_state_dict()\n",
    "        ckpoint = torch.load(model_name)\n",
    "        self.model.load_state_dict(ckpoint['model'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        all_predictions = []\n",
    "\n",
    "        data_iter = tqdm.tqdm(enumerate(test_dataloader),\n",
    "                              desc=\"Predicting\",\n",
    "                              total=len(test_dataloader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in data_iter:\n",
    "                input_ids = data['text'].to(self.device)\n",
    "\n",
    "                outputs = self.model(input_ids)\n",
    "\n",
    "                result = torch.max(outputs, 1)[1].cpu().numpy()\n",
    "                all_predictions.extend(result)\n",
    "\n",
    "        if save_file:\n",
    "            df = pd.DataFrame()\n",
    "            df['label'] = all_predictions\n",
    "            df.to_csv(self.state_dict_path + \"/\" + self.prefix_model_name + '_{:.4f}.csv'.format(ckpoint['acc']), index=None)\n",
    "\n",
    "        return all_predictions\n",
    "\n",
    "    def save_state_dict(self, epoch, loss, acc):\n",
    "        if not os.path.exists(self.state_dict_path):\n",
    "            os.mkdir(self.state_dict_path)\n",
    "\n",
    "        save_path = self.state_dict_path + '/' + self.prefix_model_name + '.epoch.{}'.format(str(epoch))\n",
    "        self.model.to('cpu')\n",
    "\n",
    "        checkpoint = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'loss': loss,\n",
    "            'acc': acc\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, save_path)\n",
    "\n",
    "        print(\"{} saved in epoch:{}\".format(save_path, epoch))\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def load_model(self):\n",
    "        checkpoint_dir = self.find_most_recent_state_dict()\n",
    "\n",
    "        checkpoint = torch.load(checkpoint_dir)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint['model'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        print(f\"{checkpoint_dir} loaded, epoch={start_epoch}\")\n",
    "        return start_epoch\n",
    "\n",
    "    def find_most_recent_state_dict(self):\n",
    "        \"\"\"\n",
    "        :param dir_path: 存储所有模型文件的目录\n",
    "        :return: 返回最新的模型文件路径, 按模型名称最后一位数进行排序\n",
    "        \"\"\"\n",
    "        dic_lis = [i for i in os.listdir(self.state_dict_path)]\n",
    "        dic_lis = [i for i in dic_lis if r\"model.epoch.\" in i]\n",
    "        if len(dic_lis) == 0:\n",
    "            raise FileNotFoundError(\"can not find any state dict in {}!\".format(self.state_dict_path))\n",
    "        dic_lis = sorted(dic_lis, key=lambda k: int(k.split(\".\")[-1]))\n",
    "        return self.state_dict_path + \"/\" + dic_lis[-1]\n",
    "\n",
    "    # 判断是否有保存的模型\n",
    "    def check_checkpoint(self):\n",
    "        if os.path.exists(self.state_dict_path):\n",
    "            dic_lis = [i for i in os.listdir(self.state_dict_path)]\n",
    "            dic_lis = [i for i in dic_lis if r\"model.epoch.\" in i]\n",
    "            if len(dic_lis) != 0:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-29\n",
    "### Counterdown  \n",
    "~~2020-12-30(+1)~~  \n",
    "~~2020-12-31(+2)~~  \n",
    "~~2020-01-02(+4)~~  \n",
    "2020-01-05(+7)  \n",
    "2020-01-13(+15)  \n",
    "2021-01-29(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python几个特殊参数 \n",
    "\n",
    "- / : 在此之前的位置不能使用关键字传参 （python3.8+）  \n",
    "def f1(a, b, /):\n",
    "     return a+b    \n",
    "     如果你想要函数的调用者在某个参数位置只能使用位置参数而不能使用关键字参数传参，那么你只需要在所需位置后面放置一个/。\n",
    "     对于上面这个函数而言，调用f1时参数a，b只能是特定的值，而不能以关键字传参，即f1(2, 3)执行正确而f1(a=2, 3)和f1(2, b=3)将执行错误。\n",
    "- : *在此之后的必须使用关键字传参\n",
    "\n",
    "### 总结： /之前的参数必须使用位置参数, *之后的必须使用关键字传参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(a, *, b, c=5):\n",
    "    return a+b+c\n",
    "\n",
    "f2(2, b=2, c=3)   # b和c赋值时必须使用b=., c=.的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个星号：元组参数  \n",
    "两个星号：字典参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(num, *args, **kwargs):\n",
    "    print(num)\n",
    "    print(args)\n",
    "    print(kwargs)\n",
    "    \n",
    "demo(1, 2, 3, name='xiaoming', gender='True')\n",
    "print('-----------------')\n",
    "gl_nums = (1, 2, 3)\n",
    "gl_xiaoming = {\"name\": \"小明\", \"age\": 18}\n",
    "\n",
    "demo(gl_nums, gl_xiaoming)  # 错误的使用方法\n",
    "\n",
    "print('-----------------')\n",
    "demo(*gl_nums, **gl_xiaoming)  # 正确的使用方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020-12-30\n",
    "### Counterdown  \n",
    "~~2020-01-03(+4)  \n",
    "2020-01-04(+2)~~  \n",
    "2020-01-06(+7)  \n",
    "2020-01-14(+15)  \n",
    "2021-01-30(+30)\n",
    "\n",
    "### 根据word2vec.txt构造一个embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "word2vec_path = '../corpus/tianchi_news/word2vec.txt'\n",
    "\n",
    "with open(word2vec_path, encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    item = lines[0].strip().split()\n",
    "    num_words, emb_size = int(item[0]), int(item[1])\n",
    "\n",
    "\n",
    "id2word = ['<pad>', '<unk>']\n",
    "index = len(id2word)\n",
    "embeddings = np.zeros((num_words + index, emb_size), dtype=np.float)\n",
    "\n",
    "for i, text in enumerate(lines[1:], index):\n",
    "    text = text.strip().split()\n",
    "    word = text[0]\n",
    "    embed = np.array(text[1:], dtype=np.float)\n",
    "    \n",
    "    id2word.append(text[0])\n",
    "    embeddings[1] += embed\n",
    "    embeddings[i] = embed\n",
    "    \n",
    "embeddings[1] = embeddings[1]/num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Embedding()的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# padding_idx的使用, 指定的索引位置初始化为0\n",
    "\n",
    "emb = torch.nn.Embedding(3, 5, padding_idx=0)\n",
    "print(emb.weight.data)\n",
    "print(emb.weight.data.requires_grad)\n",
    "\n",
    "\n",
    "pretrained_data = np.random.randn(5, 3).astype(np.float32)\n",
    "\n",
    "premd = torch.nn.Embedding.from_pretrained(torch.tensor(pretrained_data), padding_idx=0)\n",
    "print(premd.weight.data)\n",
    "print(premd.weight.data.requires_grad)\n",
    "\n",
    "premd.weight.requires_grad = True\n",
    "print(premd.weight.data.requires_grad)\n",
    "print(premd.weight.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy两种数组操作\n",
    "\n",
    "assume_unique： 指示arr1和arr2中的数据都是唯一的,设置为True可以加快运算速度\n",
    "- np.setdiff1d(arr1, arr2, assume_unique=False)   返回arr1中存在但arr2中不存在的数\n",
    "- np.append(arr1, num)  追究操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.append([1, 2, 3], 4))\n",
    "\n",
    "arr1 = [1, 2, 4, 6]\n",
    "arr2 = [1, 4, 9]\n",
    "\n",
    "np.setdiff1d(arr1, arr2, assume_unique=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021 - 01 - 02\n",
    "### Counterdown  \n",
    "~~2021-01-03(+1)  \n",
    "2021-01-04(+2)~~  \n",
    "2021-01-06(+4)  \n",
    "2021-01-09(+7)  \n",
    "2021-01-17(+15)  \n",
    "2021-02-02(+30)\n",
    "\n",
    "## numpy产生随机数的函数\n",
    "### numpy.random.randn(d0, d1, ..., dn): 返回一个标准整体分布的数据, d0..表示生成数据的维度\n",
    "\n",
    "- Return a sample (or samples) from the “standard normal” distribution.\n",
    "\n",
    "- For random samples from N($\\mu$, $\\sigma^2$), use:  $\\sigma$ * np.random.randn(...) + $\\mu$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-by-four array of samples from N(3, 6.25)\n",
    "\n",
    "3 + 2.5 * np.random.randn(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.random.rand(d0, d1, ..., dn): 返回[0, 1)均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.random.randint(low, high=None, size=None, dtype=int): 返回从[low, high)的整数  \n",
    "- Return random integers from low (inclusive) to high (exclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(2, size=10)  # 一维的10个数据\n",
    "np.random.randint(5, size=(2, 4))\n",
    "\n",
    "# 生成2*4的数据, 每一维的最小值分别为2 2 3 4, 第一维的最大值为10, 第二维的最大值为20\n",
    "np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.random.random(size=None): 返回[0, 1)的浮点数\n",
    "- return random floats in the half-open interval [0.0, 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.random((3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021 - 01 - 06\n",
    "### Counterdown  \n",
    "2021-01-07(+1)  \n",
    "2021-01-08(+2)  \n",
    "2021-01-10(+4)  \n",
    "2021-01-13(+7)  \n",
    "2021-01-21(+15)  \n",
    "2021-02-06(+30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用fasttext进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "import fasttext\n",
    "\n",
    "\n",
    "# 转换为FastText需要的格式\n",
    "train_df = pd.read_csv('../corpus/tianchi_news/train_set.csv', sep='\\t', nrows=15000)\n",
    "train_df['label_ft'] = '__label__' + train_df['label'].astype(str)\n",
    "train_df[['text','label_ft']].iloc[:-5000].to_csv('train.csv', index=None, header=None, sep='\\t')\n",
    "\n",
    "\n",
    "model = fasttext.train_supervised('train.csv', lr=1.0, wordNgrams=2, \n",
    "                                  verbose=2, minCount=1, epoch=25, loss=\"hs\")\n",
    "\n",
    "val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-5000:]['text']]\n",
    "print(f1_score(train_df['label'].values[-5000:].astype(str), val_pred, average='macro'))\n",
    "# 0.82"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
